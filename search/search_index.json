{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started : Overview of common workflows and examples. Reference : Detailed documentation of each script and it's respective functions and classes. Lessons All the MLOps lessons can be found here .","title":"TagIfAI"},{"location":"#lessons","text":"All the MLOps lessons can be found here .","title":"Lessons"},{"location":"coming_soon/","text":"This content will be coming soon! In the meantime: subscribe to our monthly newsletter follow us on Twitter and LinkedIn for updates and tips.","title":"Coming soon"},{"location":"getting_started/","text":"Use existing model Set up environment. export venv_name = \"venv\" make venv name = ${ venv_name } env = \"prod\" source ${ venv_name } /bin/activate Pull latest model. dvc pull experiments tagifai fix-artifact-metadata Run Application make app env = \"dev\" You can interact with the API directly or explore via the generated documentation at http://0.0.0.0:5000/docs . Update model (CI/CD) Coming soon after CI/CD lesson where the entire application will be retrained and deployed when we push new data (or trigger manual reoptimization/training). The deployed model, with performance comparisons to previously deployed versions, will be ready on a PR to push to the main branch. Update model (manual) Set up the development environment. export venv_name = \"venv\" make venv name = ${ venv_name } env = \"dev\" source ${ venv_name } /bin/activate Pull versioned data. dvc pull data/tags.json dvc pull data/projects.json Optimize using distributions specified in tagifai.main.objective . This also writes the best model's params to config/params.json tagifai optimize \\ --params-fp config/params.json \\ --study-name optimization \\ --num-trials 100 We'll cover how to train using compute instances on the cloud from Amazon Web Services (AWS) or Google Cloud Platforms (GCP) in later lessons. But in the meantime, if you don't have access to GPUs, check out the optimize.ipynb notebook for how to train on Colab and transfer to local. We essentially run optimization, then train the best model to download and transfer it's artifacts. Train a model (and save all it's artifacts) using params from config/params.json and publish metrics to metrics/performance.json . You can view the entire run's details inside experiments/{experiment_id}/{run_id} or via the API ( GET /runs/{run_id}). tagifai train-model \\ --params-fp config/params.json \\ --experiment-name best \\ --run-name model \\ --publish-metrics # save to metrics/performance.json Predict tags for an input sentence. It'll use the best model saved from train-model but you can also specify a run-id to choose a specific model. tagifai predict-tags --text \"Transfer learning with BERT\" # test with CLI app make app env = \"dev\" # run API and test as well View improvements Once you're done training the best model using the current data version, best hyperparameters, etc., we can view performance difference. tagifai diff Commit to git This will clean and update versioned assets (data, experiments), run tests, styling, etc. git add . git commit -m \"\" git tag -a <TAG_NAME> -m \"\" git push origin <BRANCH_NAME>","title":"Getting started"},{"location":"getting_started/#use-existing-model","text":"Set up environment. export venv_name = \"venv\" make venv name = ${ venv_name } env = \"prod\" source ${ venv_name } /bin/activate Pull latest model. dvc pull experiments tagifai fix-artifact-metadata Run Application make app env = \"dev\" You can interact with the API directly or explore via the generated documentation at http://0.0.0.0:5000/docs .","title":"Use existing model"},{"location":"getting_started/#update-model-cicd","text":"Coming soon after CI/CD lesson where the entire application will be retrained and deployed when we push new data (or trigger manual reoptimization/training). The deployed model, with performance comparisons to previously deployed versions, will be ready on a PR to push to the main branch.","title":"Update model (CI/CD)"},{"location":"getting_started/#update-model-manual","text":"Set up the development environment. export venv_name = \"venv\" make venv name = ${ venv_name } env = \"dev\" source ${ venv_name } /bin/activate Pull versioned data. dvc pull data/tags.json dvc pull data/projects.json Optimize using distributions specified in tagifai.main.objective . This also writes the best model's params to config/params.json tagifai optimize \\ --params-fp config/params.json \\ --study-name optimization \\ --num-trials 100 We'll cover how to train using compute instances on the cloud from Amazon Web Services (AWS) or Google Cloud Platforms (GCP) in later lessons. But in the meantime, if you don't have access to GPUs, check out the optimize.ipynb notebook for how to train on Colab and transfer to local. We essentially run optimization, then train the best model to download and transfer it's artifacts. Train a model (and save all it's artifacts) using params from config/params.json and publish metrics to metrics/performance.json . You can view the entire run's details inside experiments/{experiment_id}/{run_id} or via the API ( GET /runs/{run_id}). tagifai train-model \\ --params-fp config/params.json \\ --experiment-name best \\ --run-name model \\ --publish-metrics # save to metrics/performance.json Predict tags for an input sentence. It'll use the best model saved from train-model but you can also specify a run-id to choose a specific model. tagifai predict-tags --text \"Transfer learning with BERT\" # test with CLI app make app env = \"dev\" # run API and test as well View improvements Once you're done training the best model using the current data version, best hyperparameters, etc., we can view performance difference. tagifai diff Commit to git This will clean and update versioned assets (data, experiments), run tests, styling, etc. git add . git commit -m \"\" git tag -a <TAG_NAME> -m \"\" git push origin <BRANCH_NAME>","title":"Update model (manual)"},{"location":"app/api/","text":"Functions construct_response ( f ) Construct a JSON response for an endpoint's results. Source code in app/api.py def construct_response ( f ): \"\"\"Construct a JSON response for an endpoint's results.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ): results = f ( request , * args , ** kwargs ) # Construct response response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } # Add data if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap","title":"API"},{"location":"app/api/#app.api","text":"","title":"app.api"},{"location":"app/api/#functions","text":"","title":"Functions"},{"location":"app/api/#app.api.construct_response","text":"Construct a JSON response for an endpoint's results. Source code in app/api.py def construct_response ( f ): \"\"\"Construct a JSON response for an endpoint's results.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ): results = f ( request , * args , ** kwargs ) # Construct response response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } # Add data if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap","title":"construct_response()"},{"location":"app/cli/","text":"Functions behavioral_reevaluation ( model_dir = PosixPath ( '/home/runner/work/MLOps/MLOps/model' )) Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Parameters: Name Type Description Default model_dir Path location of model artifacts. Defaults to config.MODEL_DIR. PosixPath('/home/runner/work/MLOps/MLOps/model') Exceptions: Type Description ValueError Run id doesn't exist in experiment. Source code in app/cli.py @app . command () def behavioral_reevaluation ( model_dir : Path = config . MODEL_DIR , ): # pragma: no cover, requires changing existing runs \"\"\"Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Args: model_dir (Path): location of model artifacts. Defaults to config.MODEL_DIR. Raises: ValueError: Run id doesn't exist in experiment. \"\"\" # Generate behavioral report artifacts = main . load_artifacts ( model_dir = model_dir ) artifacts [ \"performance\" ][ \"behavioral\" ] = eval . get_behavioral_report ( artifacts = artifacts ) mlflow . log_metric ( \"behavioral_score\" , artifacts [ \"performance\" ][ \"behavioral\" ][ \"score\" ]) # Log updated performance utils . save_dict ( artifacts [ \"performance\" ], Path ( model_dir , \"performance.json\" )) compute_features ( params_fp = PosixPath ( '/home/runner/work/MLOps/MLOps/config/params.json' )) Compute and save features for training. Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/MLOps/MLOps/config/params.json') Source code in app/cli.py @app . command () def compute_features ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), ) -> None : \"\"\"Compute and save features for training. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Compute features main . compute_features ( params = params ) logger . info ( \"\u2705 Computed features!\" ) diff ( author = 'GokuMohandas' , repo = 'mlops' , tag_a = 'workspace' , tag_b = '' ) Difference between two release TAGs. Source code in app/cli.py @app . command () def diff ( author : str = config . AUTHOR , repo : str = config . REPO , tag_a : str = \"workspace\" , tag_b : str = \"\" , ): # pragma: no cover, can't be certain what diffs will exist \"\"\"Difference between two release TAGs.\"\"\" # Tag b if tag_b == \"\" : tags_url = f \"https://api.github.com/repos/ { author } / { repo } /tags\" tag_b = utils . load_json_from_url ( url = tags_url )[ 0 ][ \"name\" ] logger . info ( f \"Comparing { tag_a } with { tag_b } :\" ) # Params params_a = params ( author = author , repo = repo , tag = tag_a , verbose = False ) params_b = params ( author = author , repo = repo , tag = tag_b , verbose = False ) params_diff = utils . dict_diff ( d_a = params_a , d_b = params_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Parameter differences: { json . dumps ( params_diff , indent = 2 ) } \" ) # Performance performance_a = performance ( author = author , repo = repo , tag = tag_a , verbose = False ) performance_b = performance ( author = author , repo = repo , tag = tag_b , verbose = False ) performance_diff = utils . dict_diff ( d_a = performance_a , d_b = performance_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Performance differences: { json . dumps ( performance_diff , indent = 2 ) } \" ) return params_diff , performance_diff download_data () Load data from URL and save to local drive. Source code in app/cli.py @app . command () def download_data (): \"\"\"Load data from URL and save to local drive.\"\"\" # Download data projects_url = ( \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\" ) tags_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\" projects = utils . load_json_from_url ( url = projects_url ) tags = utils . load_json_from_url ( url = tags_url ) # Save data projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) utils . save_dict ( d = projects , filepath = projects_fp ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Data downloaded!\" ) get_historical_features () Retrieve historical features for training. Source code in app/cli.py @app . command () def get_historical_features (): \"\"\"Retrieve historical features for training.\"\"\" # Entities to pull data for (should dynamically read this from somewhere) project_ids = [ 1 , 2 , 3 ] now = datetime . now () timestamps = [ datetime ( now . year , now . month , now . day )] * len ( project_ids ) entity_df = pd . DataFrame . from_dict ({ \"id\" : project_ids , \"event_timestamp\" : timestamps }) # Get historical features store = FeatureStore ( repo_path = Path ( config . BASE_DIR , \"features\" )) training_df = store . get_historical_features ( entity_df = entity_df , feature_refs = [ \"project_details:text\" , \"project_details:tags\" ], ) . to_df () # Store in location for training task to pick up print ( training_df . head ()) optimize ( params_fp = PosixPath ( '/home/runner/work/MLOps/MLOps/config/params.json' ), study_name = 'optimization' , num_trials = 100 ) Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into config/params.json . Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/MLOps/MLOps/config/params.json') study_name Optional[str] Name of the study to save trial runs under. Defaults to optimization . 'optimization' num_trials int Number of trials to run. Defaults to 100. 100 Source code in app/cli.py @app . command () def optimize ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), study_name : Optional [ str ] = \"optimization\" , num_trials : int = 100 , ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into `config/params.json`. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. study_name (str, optional): Name of the study to save trial runs under. Defaults to `optimization`. num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = study_name , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : main . objective ( params , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"value\" ], ascending = False ) # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** params . __dict__ , ** study . best_trial . params } params [ \"threshold\" ] = study . best_trial . user_attrs [ \"threshold\" ] utils . save_dict ( params , params_fp , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder )) params ( author = 'GokuMohandas' , repo = 'mlops' , tag = 'workspace' , verbose = True ) Configured parametes for a specific release TAG. Source code in app/cli.py @app . command () def params ( author : str = config . AUTHOR , repo : str = config . REPO , tag : str = \"workspace\" , verbose : bool = True , ): \"\"\"Configured parametes for a specific release TAG.\"\"\" if tag == \"workspace\" : params = utils . load_dict ( filepath = Path ( config . MODEL_DIR , \"params.json\" )) else : # pragma: no cover, project specific condition url = f \"https://raw.githubusercontent.com/ { author } / { repo } / { tag } /model/params.json\" params = utils . load_json_from_url ( url = url ) if verbose : logger . info ( json . dumps ( params , indent = 2 )) return params performance ( author = 'GokuMohandas' , repo = 'mlops' , tag = 'workspace' , verbose = True ) Performance for a given release TAG in the project repo. Source code in app/cli.py @app . command () def performance ( author : str = config . AUTHOR , repo : str = config . REPO , tag : str = \"workspace\" , verbose : bool = True , ): \"\"\"Performance for a given release TAG in the project repo.\"\"\" if tag == \"workspace\" : performance = utils . load_dict ( filepath = Path ( config . MODEL_DIR , \"performance.json\" )) else : # pragma: no cover, project specific condition url = f \"https://raw.githubusercontent.com/ { author } / { repo } / { tag } /model/performance.json\" performance = utils . load_json_from_url ( url = url ) if verbose : logger . info ( json . dumps ( performance , indent = 2 )) return performance predict_tags ( text = 'Transfer learning with BERT for self-supervised learning' , run_id = 'dce5cc211fbb474e9b86af40939be0ca' ) Predict tags for a give input text using a trained model. Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text Optional[str] Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". 'Transfer learning with BERT for self-supervised learning' run_id str ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. 'dce5cc211fbb474e9b86af40939be0ca' Exceptions: Type Description ValueError Run id doesn't exist in experiment. Returns: Type Description Dict Predicted tags for input text. Source code in app/cli.py @app . command () def predict_tags ( text : Optional [ str ] = \"Transfer learning with BERT for self-supervised learning\" , run_id : str = open ( Path ( config . MODEL_DIR , \"run_id.txt\" )) . read (), ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str, optional): Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". run_id (str): ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. Raises: ValueError: Run id doesn't exist in experiment. Returns: Predicted tags for input text. \"\"\" # Predict artifacts = main . load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction train_model ( params_fp = PosixPath ( '/home/runner/work/MLOps/MLOps/config/params.json' ), model_dir = PosixPath ( '/home/runner/work/MLOps/MLOps/model' ), experiment_name = 'best' , run_name = 'model' ) Train a model using the specified parameters. Parameters: Name Type Description Default params_fp Path Parameters to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/MLOps/MLOps/config/params.json') model_dir Optional[pathlib.Path] location of model artifacts. Defaults to config.MODEL_DIR. PosixPath('/home/runner/work/MLOps/MLOps/model') experiment_name Optional[str] Name of the experiment to save the run to. Defaults to best . 'best' run_name Optional[str] Name of the run. Defaults to model . 'model' Source code in app/cli.py @app . command () def train_model ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), model_dir : Optional [ Path ] = Path ( config . MODEL_DIR ), experiment_name : Optional [ str ] = \"best\" , run_name : Optional [ str ] = \"model\" , ) -> None : \"\"\"Train a model using the specified parameters. Args: params_fp (Path, optional): Parameters to use for training. Defaults to `config/params.json`. model_dir (Path): location of model artifacts. Defaults to config.MODEL_DIR. experiment_name (str, optional): Name of the experiment to save the run to. Defaults to `best`. run_name (str, optional): Name of the run. Defaults to `model`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Start run mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id # Train artifacts = main . train_model ( params = params ) # Set tags tags = {} mlflow . set_tags ( tags ) # Log metrics performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) metrics = { \"precision\" : performance [ \"overall\" ][ \"precision\" ], \"recall\" : performance [ \"overall\" ][ \"recall\" ], \"f1\" : performance [ \"overall\" ][ \"f1\" ], \"best_val_loss\" : artifacts [ \"loss\" ], \"behavioral_score\" : performance [ \"behavioral\" ][ \"score\" ], \"slices_f1\" : performance [ \"slices\" ][ \"overall\" ][ \"f1\" ], } mlflow . log_metrics ( metrics ) # Log artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"params\" ]), Path ( dp , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) artifacts [ \"tokenizer\" ] . save ( Path ( dp , \"tokenizer.json\" )) torch . save ( artifacts [ \"model\" ] . state_dict (), Path ( dp , \"model.pt\" )) mlflow . log_artifacts ( dp ) mlflow . log_params ( vars ( artifacts [ \"params\" ])) # Save for repo open ( Path ( model_dir , \"run_id.txt\" ), \"w\" ) . write ( run_id ) utils . save_dict ( vars ( params ), Path ( model_dir , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( model_dir , \"performance.json\" ))","title":"CLI"},{"location":"app/cli/#app.cli","text":"","title":"app.cli"},{"location":"app/cli/#functions","text":"","title":"Functions"},{"location":"app/cli/#app.cli.behavioral_reevaluation","text":"Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Parameters: Name Type Description Default model_dir Path location of model artifacts. Defaults to config.MODEL_DIR. PosixPath('/home/runner/work/MLOps/MLOps/model') Exceptions: Type Description ValueError Run id doesn't exist in experiment. Source code in app/cli.py @app . command () def behavioral_reevaluation ( model_dir : Path = config . MODEL_DIR , ): # pragma: no cover, requires changing existing runs \"\"\"Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Args: model_dir (Path): location of model artifacts. Defaults to config.MODEL_DIR. Raises: ValueError: Run id doesn't exist in experiment. \"\"\" # Generate behavioral report artifacts = main . load_artifacts ( model_dir = model_dir ) artifacts [ \"performance\" ][ \"behavioral\" ] = eval . get_behavioral_report ( artifacts = artifacts ) mlflow . log_metric ( \"behavioral_score\" , artifacts [ \"performance\" ][ \"behavioral\" ][ \"score\" ]) # Log updated performance utils . save_dict ( artifacts [ \"performance\" ], Path ( model_dir , \"performance.json\" ))","title":"behavioral_reevaluation()"},{"location":"app/cli/#app.cli.compute_features","text":"Compute and save features for training. Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/MLOps/MLOps/config/params.json') Source code in app/cli.py @app . command () def compute_features ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), ) -> None : \"\"\"Compute and save features for training. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Compute features main . compute_features ( params = params ) logger . info ( \"\u2705 Computed features!\" )","title":"compute_features()"},{"location":"app/cli/#app.cli.diff","text":"Difference between two release TAGs. Source code in app/cli.py @app . command () def diff ( author : str = config . AUTHOR , repo : str = config . REPO , tag_a : str = \"workspace\" , tag_b : str = \"\" , ): # pragma: no cover, can't be certain what diffs will exist \"\"\"Difference between two release TAGs.\"\"\" # Tag b if tag_b == \"\" : tags_url = f \"https://api.github.com/repos/ { author } / { repo } /tags\" tag_b = utils . load_json_from_url ( url = tags_url )[ 0 ][ \"name\" ] logger . info ( f \"Comparing { tag_a } with { tag_b } :\" ) # Params params_a = params ( author = author , repo = repo , tag = tag_a , verbose = False ) params_b = params ( author = author , repo = repo , tag = tag_b , verbose = False ) params_diff = utils . dict_diff ( d_a = params_a , d_b = params_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Parameter differences: { json . dumps ( params_diff , indent = 2 ) } \" ) # Performance performance_a = performance ( author = author , repo = repo , tag = tag_a , verbose = False ) performance_b = performance ( author = author , repo = repo , tag = tag_b , verbose = False ) performance_diff = utils . dict_diff ( d_a = performance_a , d_b = performance_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Performance differences: { json . dumps ( performance_diff , indent = 2 ) } \" ) return params_diff , performance_diff","title":"diff()"},{"location":"app/cli/#app.cli.download_data","text":"Load data from URL and save to local drive. Source code in app/cli.py @app . command () def download_data (): \"\"\"Load data from URL and save to local drive.\"\"\" # Download data projects_url = ( \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\" ) tags_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\" projects = utils . load_json_from_url ( url = projects_url ) tags = utils . load_json_from_url ( url = tags_url ) # Save data projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) utils . save_dict ( d = projects , filepath = projects_fp ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Data downloaded!\" )","title":"download_data()"},{"location":"app/cli/#app.cli.get_historical_features","text":"Retrieve historical features for training. Source code in app/cli.py @app . command () def get_historical_features (): \"\"\"Retrieve historical features for training.\"\"\" # Entities to pull data for (should dynamically read this from somewhere) project_ids = [ 1 , 2 , 3 ] now = datetime . now () timestamps = [ datetime ( now . year , now . month , now . day )] * len ( project_ids ) entity_df = pd . DataFrame . from_dict ({ \"id\" : project_ids , \"event_timestamp\" : timestamps }) # Get historical features store = FeatureStore ( repo_path = Path ( config . BASE_DIR , \"features\" )) training_df = store . get_historical_features ( entity_df = entity_df , feature_refs = [ \"project_details:text\" , \"project_details:tags\" ], ) . to_df () # Store in location for training task to pick up print ( training_df . head ())","title":"get_historical_features()"},{"location":"app/cli/#app.cli.optimize","text":"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into config/params.json . Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/MLOps/MLOps/config/params.json') study_name Optional[str] Name of the study to save trial runs under. Defaults to optimization . 'optimization' num_trials int Number of trials to run. Defaults to 100. 100 Source code in app/cli.py @app . command () def optimize ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), study_name : Optional [ str ] = \"optimization\" , num_trials : int = 100 , ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into `config/params.json`. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. study_name (str, optional): Name of the study to save trial runs under. Defaults to `optimization`. num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = study_name , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : main . objective ( params , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"value\" ], ascending = False ) # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** params . __dict__ , ** study . best_trial . params } params [ \"threshold\" ] = study . best_trial . user_attrs [ \"threshold\" ] utils . save_dict ( params , params_fp , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder ))","title":"optimize()"},{"location":"app/cli/#app.cli.params","text":"Configured parametes for a specific release TAG. Source code in app/cli.py @app . command () def params ( author : str = config . AUTHOR , repo : str = config . REPO , tag : str = \"workspace\" , verbose : bool = True , ): \"\"\"Configured parametes for a specific release TAG.\"\"\" if tag == \"workspace\" : params = utils . load_dict ( filepath = Path ( config . MODEL_DIR , \"params.json\" )) else : # pragma: no cover, project specific condition url = f \"https://raw.githubusercontent.com/ { author } / { repo } / { tag } /model/params.json\" params = utils . load_json_from_url ( url = url ) if verbose : logger . info ( json . dumps ( params , indent = 2 )) return params","title":"params()"},{"location":"app/cli/#app.cli.performance","text":"Performance for a given release TAG in the project repo. Source code in app/cli.py @app . command () def performance ( author : str = config . AUTHOR , repo : str = config . REPO , tag : str = \"workspace\" , verbose : bool = True , ): \"\"\"Performance for a given release TAG in the project repo.\"\"\" if tag == \"workspace\" : performance = utils . load_dict ( filepath = Path ( config . MODEL_DIR , \"performance.json\" )) else : # pragma: no cover, project specific condition url = f \"https://raw.githubusercontent.com/ { author } / { repo } / { tag } /model/performance.json\" performance = utils . load_json_from_url ( url = url ) if verbose : logger . info ( json . dumps ( performance , indent = 2 )) return performance","title":"performance()"},{"location":"app/cli/#app.cli.predict_tags","text":"Predict tags for a give input text using a trained model. Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text Optional[str] Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". 'Transfer learning with BERT for self-supervised learning' run_id str ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. 'dce5cc211fbb474e9b86af40939be0ca' Exceptions: Type Description ValueError Run id doesn't exist in experiment. Returns: Type Description Dict Predicted tags for input text. Source code in app/cli.py @app . command () def predict_tags ( text : Optional [ str ] = \"Transfer learning with BERT for self-supervised learning\" , run_id : str = open ( Path ( config . MODEL_DIR , \"run_id.txt\" )) . read (), ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str, optional): Input text to predict tags for. Defaults to \"Transfer learning with BERT for self-supervised learning\". run_id (str): ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. Raises: ValueError: Run id doesn't exist in experiment. Returns: Predicted tags for input text. \"\"\" # Predict artifacts = main . load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction","title":"predict_tags()"},{"location":"app/cli/#app.cli.train_model","text":"Train a model using the specified parameters. Parameters: Name Type Description Default params_fp Path Parameters to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/MLOps/MLOps/config/params.json') model_dir Optional[pathlib.Path] location of model artifacts. Defaults to config.MODEL_DIR. PosixPath('/home/runner/work/MLOps/MLOps/model') experiment_name Optional[str] Name of the experiment to save the run to. Defaults to best . 'best' run_name Optional[str] Name of the run. Defaults to model . 'model' Source code in app/cli.py @app . command () def train_model ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), model_dir : Optional [ Path ] = Path ( config . MODEL_DIR ), experiment_name : Optional [ str ] = \"best\" , run_name : Optional [ str ] = \"model\" , ) -> None : \"\"\"Train a model using the specified parameters. Args: params_fp (Path, optional): Parameters to use for training. Defaults to `config/params.json`. model_dir (Path): location of model artifacts. Defaults to config.MODEL_DIR. experiment_name (str, optional): Name of the experiment to save the run to. Defaults to `best`. run_name (str, optional): Name of the run. Defaults to `model`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Start run mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id # Train artifacts = main . train_model ( params = params ) # Set tags tags = {} mlflow . set_tags ( tags ) # Log metrics performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) metrics = { \"precision\" : performance [ \"overall\" ][ \"precision\" ], \"recall\" : performance [ \"overall\" ][ \"recall\" ], \"f1\" : performance [ \"overall\" ][ \"f1\" ], \"best_val_loss\" : artifacts [ \"loss\" ], \"behavioral_score\" : performance [ \"behavioral\" ][ \"score\" ], \"slices_f1\" : performance [ \"slices\" ][ \"overall\" ][ \"f1\" ], } mlflow . log_metrics ( metrics ) # Log artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"params\" ]), Path ( dp , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) artifacts [ \"tokenizer\" ] . save ( Path ( dp , \"tokenizer.json\" )) torch . save ( artifacts [ \"model\" ] . state_dict (), Path ( dp , \"model.pt\" )) mlflow . log_artifacts ( dp ) mlflow . log_params ( vars ( artifacts [ \"params\" ])) # Save for repo open ( Path ( model_dir , \"run_id.txt\" ), \"w\" ) . write ( run_id ) utils . save_dict ( vars ( params ), Path ( model_dir , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( model_dir , \"performance.json\" ))","title":"train_model()"},{"location":"app/config/","text":"In this file we're setting up the configuration needed for all our workflows. First up is creating required directories so we can save and load from them: # Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) MODEL_REGISTRY = Path ( BASE_DIR , \"experiments\" ) DVC_REMOTE_STORAGE = Path ( BASE_DIR , \"tmp/dvcstore\" ) # Create dirs LOGS_DIR . mkdir ( parents = True , exist_ok = True ) DATA_DIR . mkdir ( parents = True , exist_ok = True ) MODEL_REGISTRY . mkdir ( parents = True , exist_ok = True ) DVC_REMOTE_STORAGE . mkdir ( parents = True , exist_ok = True ) Then, we'll set the tracking URI for all MLFlow experiments: # MLFlow mlflow . set_tracking_uri ( \"file://\" + str ( MODEL_REGISTRY . absolute ())) Finally, we'll establish our logger using the logging_config dictionary: # Logger logging . config . dictConfig ( logging_config ) logger = logging . getLogger ( \"root\" ) logger . handlers [ 0 ] = RichHandler ( markup = True )","title":"Config"},{"location":"app/config/#app.config","text":"","title":"app.config"},{"location":"tagifai/data/","text":"Classes CNNTextDataset Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Methods collate_fn ( self , batch ) Processing on a batch. It's used to override the default collate_fn in torch.utils.data.DataLoader . Parameters: Name Type Description Default batch List List of inputs and outputs. required Returns: Type Description Tuple Processed inputs and outputs. Source code in tagifai/data.py def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y create_dataloader ( self , batch_size , shuffle = False , drop_last = False ) Create dataloaders to load batches with. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Parameters: Name Type Description Default batch_size int Number of samples per batch. required shuffle bool Shuffle each batch. Defaults to False. False drop_last bool Drop the last batch if it's less than batch_size . Defaults to False. False Returns: Type Description DataLoader Torch dataloader to load batches with. Source code in tagifai/data.py def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , ) LabelEncoder Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) MultiClassLabelEncoder Encode labels into unique indices for multi-class classification. Methods decode ( self , y ) Decode a collection of class indices. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes encode ( self , y ) Encode a collection of classes. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self MultiLabelLabelEncoder Encode labels into unique indices for multi-label classification. Methods decode ( self , y ) Decode a (multilabel) one-hot encoding into corresponding labels. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes encode ( self , y ) Encode a collection of labels using (multilabel) one-hot encoding. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self Stemmer Methods stem ( self , word ) Strip affixes from the token and return the stem. :param token: The token that should be stemmed. :type token: str Source code in tagifai/data.py def stem ( self , word ): if self . mode == self . NLTK_EXTENSIONS and word in self . pool : # pragma: no cover, nltk return self . pool [ word ] if self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 : # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem Tokenizer Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ), dtype = object ) Methods fit_on_texts ( self , texts ) Learn token mappings from a list of texts. Parameters: Name Type Description Default texts List List of texts made of tokens. required Source code in tagifai/data.py def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self sequences_to_texts ( self , sequences ) Convert a lists of arrays of indices to a list of texts. Parameters: Name Type Description Default sequences List list of mapped tokens to convert back to text. required Returns: Type Description List Mapped text from index tokens. Source code in tagifai/data.py def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts texts_to_sequences ( self , texts ) Convert a list of texts to a lists of arrays of indices. Parameters: Name Type Description Default texts List List of texts to tokenize and map to indices. required Returns: Type Description List[List] A list of mapped sequences (list of indices). Source code in tagifai/data.py def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ])) sequences . append ( sequence ) return sequences Functions filter_items ( items , include = [], exclude = []) Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = list ( tags_dict . keys ()), exclude = config . EXCLDUE , ) Source code in tagifai/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Filter tags for each project df.tags = df.tags.apply( filter_items, include=list(tags_dict.keys()), exclude=config.EXCLDUE, ) ``` \"\"\" # Filter filtered = [ item for item in items if item in include and item not in exclude ] return filtered iterative_train_test_split ( X , y , train_size = 0.7 ) Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X Series Input features as a pandas Series object. required y ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in tagifai/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test pad_sequences ( sequences , max_seq_len = 0 ) Zero pad sequences to a specified max_seq_len or to the length of the largest sequence in sequences . Usage: # Pad inputs seq = np . array ([[ 1 , 2 , 3 ], [ 1 , 2 ]], dtype = object ) padded_seq = pad_sequences ( sequences = seq , max_seq_len = 5 ) print ( padded_seq ) [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] Note Input sequences must be 2D. Check out this implemention for a more generalized approach. Parameters: Name Type Description Default sequences ndarray 2D array of data to be padded. required max_seq_len int Length to pad sequences to. Defaults to 0. 0 Exceptions: Type Description ValueError Input sequences are not two-dimensional. Returns: Type Description ndarray An array with the zero padded sequences. Source code in tagifai/data.py def pad_sequences ( sequences : np . ndarray , max_seq_len : int = 0 ) -> np . ndarray : \"\"\"Zero pad sequences to a specified `max_seq_len` or to the length of the largest sequence in `sequences`. Usage: ```python # Pad inputs seq = np.array([[1, 2, 3], [1, 2]], dtype=object) padded_seq = pad_sequences(sequences=seq, max_seq_len=5) print (padded_seq) ``` <pre> [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] </pre> Note: Input `sequences` must be 2D. Check out this [implemention](https://madewithml.com/courses/foundations/convolutional-neural-networks/#padding){:target=\"_blank\"} for a more generalized approach. Args: sequences (np.ndarray): 2D array of data to be padded. max_seq_len (int, optional): Length to pad sequences to. Defaults to 0. Raises: ValueError: Input sequences are not two-dimensional. Returns: An array with the zero padded sequences. \"\"\" # Get max sequence length max_seq_len = max ( max_seq_len , max ( len ( sequence ) for sequence in sequences )) # Pad padded_sequences = np . zeros (( len ( sequences ), max_seq_len )) for i , sequence in enumerate ( sequences ): padded_sequences [ i ][: len ( sequence )] = sequence return padded_sequences prepare ( df , include = [], exclude = [], min_tag_freq = 30 ) Prepare the raw data. Parameters: Name Type Description Default df DataFrame Pandas DataFrame with data. required include List list of tags to include. [] exclude List list of tags to exclude. [] min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe and dictionary of tags and counts above the frequency threshold. Source code in tagifai/data.py def prepare ( df : pd . DataFrame , include : List = [], exclude : List = [], min_tag_freq : int = 30 ) -> Tuple : \"\"\"Prepare the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. include (List): list of tags to include. exclude (List): list of tags to exclude. min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe and dictionary of tags and counts above the frequency threshold. \"\"\" # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) tags_below_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] < min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_above_freq , tags_below_freq preprocess ( text , lower = True , stem = False , filters = '[! \\\\ \" \\' #$%&()* \\\\ +,-./:;<=>?@ \\\\\\\\\\\\ [ \\\\ ]^_`{|}~]' , stopwords = [ 'i' , 'me' , 'my' , 'myself' , 'we' , 'our' , 'ours' , 'ourselves' , 'you' , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , 'your' , 'yours' , 'yourself' , 'yourselves' , 'he' , 'him' , 'his' , 'himself' , 'she' , \"she's\" , 'her' , 'hers' , 'herself' , 'it' , \"it's\" , 'its' , 'itself' , 'they' , 'them' , 'their' , 'theirs' , 'themselves' , 'what' , 'which' , 'who' , 'whom' , 'this' , 'that' , \"that'll\" , 'these' , 'those' , 'am' , 'is' , 'are' , 'was' , 'were' , 'be' , 'been' , 'being' , 'have' , 'has' , 'had' , 'having' , 'do' , 'does' , 'did' , 'doing' , 'a' , 'an' , 'the' , 'and' , 'but' , 'if' , 'or' , 'because' , 'as' , 'until' , 'while' , 'of' , 'at' , 'by' , 'for' , 'with' , 'about' , 'against' , 'between' , 'into' , 'through' , 'during' , 'before' , 'after' , 'above' , 'below' , 'to' , 'from' , 'up' , 'down' , 'in' , 'out' , 'on' , 'off' , 'over' , 'under' , 'again' , 'further' , 'then' , 'once' , 'here' , 'there' , 'when' , 'where' , 'why' , 'how' , 'all' , 'any' , 'both' , 'each' , 'few' , 'more' , 'most' , 'other' , 'some' , 'such' , 'no' , 'nor' , 'not' , 'only' , 'own' , 'same' , 'so' , 'than' , 'too' , 'very' , 's' , 't' , 'can' , 'will' , 'just' , 'don' , \"don't\" , 'should' , \"should've\" , 'now' , 'd' , 'll' , 'm' , 'o' , 're' , 've' , 'y' , 'ain' , 'aren' , \"aren't\" , 'couldn' , \"couldn't\" , 'didn' , \"didn't\" , 'doesn' , \"doesn't\" , 'hadn' , \"hadn't\" , 'hasn' , \"hasn't\" , 'haven' , \"haven't\" , 'isn' , \"isn't\" , 'ma' , 'mightn' , \"mightn't\" , 'mustn' , \"mustn't\" , 'needn' , \"needn't\" , 'shan' , \"shan't\" , 'shouldn' , \"shouldn't\" , 'wasn' , \"wasn't\" , 'weren' , \"weren't\" , 'won' , \"won't\" , 'wouldn' , \"wouldn't\" ]) Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False filters str Filters to apply on text. '[!\\\\\"\\'#$%&()*\\\\+,-./:;<=>?@\\\\\\\\\\\\[\\\\]^_`{|}~]' stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in tagifai/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , filters : str = r \"[! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~]\" , stopwords : List = [ \"i\" , \"me\" , \"my\" , \"myself\" , \"we\" , \"our\" , \"ours\" , \"ourselves\" , \"you\" , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , \"your\" , \"yours\" , \"yourself\" , \"yourselves\" , \"he\" , \"him\" , \"his\" , \"himself\" , \"she\" , \"she's\" , \"her\" , \"hers\" , \"herself\" , \"it\" , \"it's\" , \"its\" , \"itself\" , \"they\" , \"them\" , \"their\" , \"theirs\" , \"themselves\" , \"what\" , \"which\" , \"who\" , \"whom\" , \"this\" , \"that\" , \"that'll\" , \"these\" , \"those\" , \"am\" , \"is\" , \"are\" , \"was\" , \"were\" , \"be\" , \"been\" , \"being\" , \"have\" , \"has\" , \"had\" , \"having\" , \"do\" , \"does\" , \"did\" , \"doing\" , \"a\" , \"an\" , \"the\" , \"and\" , \"but\" , \"if\" , \"or\" , \"because\" , \"as\" , \"until\" , \"while\" , \"of\" , \"at\" , \"by\" , \"for\" , \"with\" , \"about\" , \"against\" , \"between\" , \"into\" , \"through\" , \"during\" , \"before\" , \"after\" , \"above\" , \"below\" , \"to\" , \"from\" , \"up\" , \"down\" , \"in\" , \"out\" , \"on\" , \"off\" , \"over\" , \"under\" , \"again\" , \"further\" , \"then\" , \"once\" , \"here\" , \"there\" , \"when\" , \"where\" , \"why\" , \"how\" , \"all\" , \"any\" , \"both\" , \"each\" , \"few\" , \"more\" , \"most\" , \"other\" , \"some\" , \"such\" , \"no\" , \"nor\" , \"not\" , \"only\" , \"own\" , \"same\" , \"so\" , \"than\" , \"too\" , \"very\" , \"s\" , \"t\" , \"can\" , \"will\" , \"just\" , \"don\" , \"don't\" , \"should\" , \"should've\" , \"now\" , \"d\" , \"ll\" , \"m\" , \"o\" , \"re\" , \"ve\" , \"y\" , \"ain\" , \"aren\" , \"aren't\" , \"couldn\" , \"couldn't\" , \"didn\" , \"didn't\" , \"doesn\" , \"doesn't\" , \"hadn\" , \"hadn't\" , \"hasn\" , \"hasn't\" , \"haven\" , \"haven't\" , \"isn\" , \"isn't\" , \"ma\" , \"mightn\" , \"mightn't\" , \"mustn\" , \"mustn't\" , \"needn\" , \"needn't\" , \"shan\" , \"shan't\" , \"shouldn\" , \"shouldn't\" , \"wasn\" , \"wasn't\" , \"weren\" , \"weren't\" , \"won\" , \"won't\" , \"wouldn\" , \"wouldn't\" , ], ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. filters (str, optional): Filters to apply on text. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords if len ( stopwords ): pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([-;;.,!?<=>])\" , r \" \\1 \" , text ) text = re . sub ( filters , r \"\" , text ) text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = Stemmer () text = \" \" . join ([ stemmer . stem ( word ) for word in text . split ( \" \" )]) return text","title":"Data"},{"location":"tagifai/data/#tagifai.data","text":"","title":"tagifai.data"},{"location":"tagifai/data/#classes","text":"","title":"Classes"},{"location":"tagifai/data/#tagifai.data.CNNTextDataset","text":"Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size )","title":"CNNTextDataset"},{"location":"tagifai/data/#methods","text":"","title":"Methods"},{"location":"tagifai/data/#tagifai.data.LabelEncoder","text":"Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels )","title":"LabelEncoder"},{"location":"tagifai/data/#tagifai.data.MultiClassLabelEncoder","text":"Encode labels into unique indices for multi-class classification.","title":"MultiClassLabelEncoder"},{"location":"tagifai/data/#methods_1","text":"","title":"Methods"},{"location":"tagifai/data/#tagifai.data.MultiLabelLabelEncoder","text":"Encode labels into unique indices for multi-label classification.","title":"MultiLabelLabelEncoder"},{"location":"tagifai/data/#methods_2","text":"","title":"Methods"},{"location":"tagifai/data/#tagifai.data.Stemmer","text":"","title":"Stemmer"},{"location":"tagifai/data/#methods_3","text":"","title":"Methods"},{"location":"tagifai/data/#tagifai.data.Tokenizer","text":"Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ), dtype = object )","title":"Tokenizer"},{"location":"tagifai/data/#methods_4","text":"","title":"Methods"},{"location":"tagifai/data/#functions","text":"","title":"Functions"},{"location":"tagifai/data/#tagifai.data.filter_items","text":"Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = list ( tags_dict . keys ()), exclude = config . EXCLDUE , ) Source code in tagifai/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Filter tags for each project df.tags = df.tags.apply( filter_items, include=list(tags_dict.keys()), exclude=config.EXCLDUE, ) ``` \"\"\" # Filter filtered = [ item for item in items if item in include and item not in exclude ] return filtered","title":"filter_items()"},{"location":"tagifai/data/#tagifai.data.iterative_train_test_split","text":"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X Series Input features as a pandas Series object. required y ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in tagifai/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test","title":"iterative_train_test_split()"},{"location":"tagifai/data/#tagifai.data.pad_sequences","text":"Zero pad sequences to a specified max_seq_len or to the length of the largest sequence in sequences . Usage: # Pad inputs seq = np . array ([[ 1 , 2 , 3 ], [ 1 , 2 ]], dtype = object ) padded_seq = pad_sequences ( sequences = seq , max_seq_len = 5 ) print ( padded_seq ) [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] Note Input sequences must be 2D. Check out this implemention for a more generalized approach. Parameters: Name Type Description Default sequences ndarray 2D array of data to be padded. required max_seq_len int Length to pad sequences to. Defaults to 0. 0 Exceptions: Type Description ValueError Input sequences are not two-dimensional. Returns: Type Description ndarray An array with the zero padded sequences. Source code in tagifai/data.py def pad_sequences ( sequences : np . ndarray , max_seq_len : int = 0 ) -> np . ndarray : \"\"\"Zero pad sequences to a specified `max_seq_len` or to the length of the largest sequence in `sequences`. Usage: ```python # Pad inputs seq = np.array([[1, 2, 3], [1, 2]], dtype=object) padded_seq = pad_sequences(sequences=seq, max_seq_len=5) print (padded_seq) ``` <pre> [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] </pre> Note: Input `sequences` must be 2D. Check out this [implemention](https://madewithml.com/courses/foundations/convolutional-neural-networks/#padding){:target=\"_blank\"} for a more generalized approach. Args: sequences (np.ndarray): 2D array of data to be padded. max_seq_len (int, optional): Length to pad sequences to. Defaults to 0. Raises: ValueError: Input sequences are not two-dimensional. Returns: An array with the zero padded sequences. \"\"\" # Get max sequence length max_seq_len = max ( max_seq_len , max ( len ( sequence ) for sequence in sequences )) # Pad padded_sequences = np . zeros (( len ( sequences ), max_seq_len )) for i , sequence in enumerate ( sequences ): padded_sequences [ i ][: len ( sequence )] = sequence return padded_sequences","title":"pad_sequences()"},{"location":"tagifai/data/#tagifai.data.prepare","text":"Prepare the raw data. Parameters: Name Type Description Default df DataFrame Pandas DataFrame with data. required include List list of tags to include. [] exclude List list of tags to exclude. [] min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe and dictionary of tags and counts above the frequency threshold. Source code in tagifai/data.py def prepare ( df : pd . DataFrame , include : List = [], exclude : List = [], min_tag_freq : int = 30 ) -> Tuple : \"\"\"Prepare the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. include (List): list of tags to include. exclude (List): list of tags to exclude. min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe and dictionary of tags and counts above the frequency threshold. \"\"\" # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) tags_below_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] < min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_above_freq , tags_below_freq","title":"prepare()"},{"location":"tagifai/data/#tagifai.data.preprocess","text":"Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False filters str Filters to apply on text. '[!\\\\\"\\'#$%&()*\\\\+,-./:;<=>?@\\\\\\\\\\\\[\\\\]^_`{|}~]' stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in tagifai/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , filters : str = r \"[! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~]\" , stopwords : List = [ \"i\" , \"me\" , \"my\" , \"myself\" , \"we\" , \"our\" , \"ours\" , \"ourselves\" , \"you\" , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , \"your\" , \"yours\" , \"yourself\" , \"yourselves\" , \"he\" , \"him\" , \"his\" , \"himself\" , \"she\" , \"she's\" , \"her\" , \"hers\" , \"herself\" , \"it\" , \"it's\" , \"its\" , \"itself\" , \"they\" , \"them\" , \"their\" , \"theirs\" , \"themselves\" , \"what\" , \"which\" , \"who\" , \"whom\" , \"this\" , \"that\" , \"that'll\" , \"these\" , \"those\" , \"am\" , \"is\" , \"are\" , \"was\" , \"were\" , \"be\" , \"been\" , \"being\" , \"have\" , \"has\" , \"had\" , \"having\" , \"do\" , \"does\" , \"did\" , \"doing\" , \"a\" , \"an\" , \"the\" , \"and\" , \"but\" , \"if\" , \"or\" , \"because\" , \"as\" , \"until\" , \"while\" , \"of\" , \"at\" , \"by\" , \"for\" , \"with\" , \"about\" , \"against\" , \"between\" , \"into\" , \"through\" , \"during\" , \"before\" , \"after\" , \"above\" , \"below\" , \"to\" , \"from\" , \"up\" , \"down\" , \"in\" , \"out\" , \"on\" , \"off\" , \"over\" , \"under\" , \"again\" , \"further\" , \"then\" , \"once\" , \"here\" , \"there\" , \"when\" , \"where\" , \"why\" , \"how\" , \"all\" , \"any\" , \"both\" , \"each\" , \"few\" , \"more\" , \"most\" , \"other\" , \"some\" , \"such\" , \"no\" , \"nor\" , \"not\" , \"only\" , \"own\" , \"same\" , \"so\" , \"than\" , \"too\" , \"very\" , \"s\" , \"t\" , \"can\" , \"will\" , \"just\" , \"don\" , \"don't\" , \"should\" , \"should've\" , \"now\" , \"d\" , \"ll\" , \"m\" , \"o\" , \"re\" , \"ve\" , \"y\" , \"ain\" , \"aren\" , \"aren't\" , \"couldn\" , \"couldn't\" , \"didn\" , \"didn't\" , \"doesn\" , \"doesn't\" , \"hadn\" , \"hadn't\" , \"hasn\" , \"hasn't\" , \"haven\" , \"haven't\" , \"isn\" , \"isn't\" , \"ma\" , \"mightn\" , \"mightn't\" , \"mustn\" , \"mustn't\" , \"needn\" , \"needn't\" , \"shan\" , \"shan't\" , \"shouldn\" , \"shouldn't\" , \"wasn\" , \"wasn't\" , \"weren\" , \"weren't\" , \"won\" , \"won't\" , \"wouldn\" , \"wouldn't\" , ], ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. filters (str, optional): Filters to apply on text. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords if len ( stopwords ): pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([-;;.,!?<=>])\" , r \" \\1 \" , text ) text = re . sub ( filters , r \"\" , text ) text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = Stemmer () text = \" \" . join ([ stemmer . stem ( word ) for word in text . split ( \" \" )]) return text","title":"preprocess()"},{"location":"tagifai/models/","text":"Classes CNN Methods __init__ ( self , embedding_dim , vocab_size , num_filters , filter_sizes , hidden_dim , dropout_p , num_classes , padding_idx = 0 ) special A convolutional neural network architecture created for natural language processing tasks where filters convolve across the given text inputs. Usage: # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) Parameters: Name Type Description Default embedding_dim int Embedding dimension for tokens. required vocab_size int Number of unique tokens in vocabulary. required num_filters int Number of filters per filter size. required filter_sizes list List of filter sizes for the CNN. required hidden_dim int Hidden dimension for fully-connected (FC) layers. required dropout_p float Dropout proportion for FC layers. required num_classes int Number of unique classes to classify into. required padding_idx int Index representing the <PAD> token. Defaults to 0. 0 Source code in tagifai/models.py def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes ) forward ( self , inputs , channel_first = False ) Forward pass. Parameters: Name Type Description Default inputs List List of inputs (by feature). required channel_first bool Channel dimension is first in inputs. Defaults to False. False Returns: Type Description Tensor Outputs from the model. Source code in tagifai/models.py def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ]) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z Functions initialize_model ( params , vocab_size , num_classes , device = device ( type = 'cpu' )) Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Module Initialize torch model instance. Source code in tagifai/models.py def initialize_model ( params : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: params (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Initialize torch model instance. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"Models"},{"location":"tagifai/models/#tagifai.models","text":"","title":"tagifai.models"},{"location":"tagifai/models/#classes","text":"","title":"Classes"},{"location":"tagifai/models/#tagifai.models.CNN","text":"","title":"CNN"},{"location":"tagifai/models/#methods","text":"","title":"Methods"},{"location":"tagifai/models/#functions","text":"","title":"Functions"},{"location":"tagifai/models/#tagifai.models.initialize_model","text":"Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Module Initialize torch model instance. Source code in tagifai/models.py def initialize_model ( params : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: params (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Initialize torch model instance. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"initialize_model()"},{"location":"tagifai/predict/","text":"Functions predict ( texts , artifacts , device = device ( type = 'cpu' )) Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] artifacts = load_artifacts ( run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) predict ( texts = texts , artifacts = artifacts ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input parameter texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input texts to predict tags for. required artifacts Dict Artifacts needed for inference. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Predicted tags for each of the input texts. Source code in tagifai/predict.py def predict ( texts : List , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] artifacts = load_artifacts(run_id=\"264ac530b78c42608e5dea1086bc2c73\") predict(texts=texts, artifacts=artifacts) ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input parameter `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input texts to predict tags for. artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Predicted tags for each of the input texts. \"\"\" # Retrieve artifacts params = artifacts [ \"params\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] model = artifacts [ \"model\" ] # Prepare data preprocessed_texts = [ data . preprocess ( text , lower = bool ( strtobool ( str ( params . lower ))), # params.lower could be str/bool stem = bool ( strtobool ( str ( params . stem ))), ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts ), dtype = \"object\" ) y_filler = np . zeros (( len ( X ), len ( label_encoder ))) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"Inference"},{"location":"tagifai/predict/#tagifai.predict","text":"","title":"tagifai.predict"},{"location":"tagifai/predict/#functions","text":"","title":"Functions"},{"location":"tagifai/predict/#tagifai.predict.predict","text":"Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] artifacts = load_artifacts ( run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) predict ( texts = texts , artifacts = artifacts ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input parameter texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input texts to predict tags for. required artifacts Dict Artifacts needed for inference. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Predicted tags for each of the input texts. Source code in tagifai/predict.py def predict ( texts : List , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] artifacts = load_artifacts(run_id=\"264ac530b78c42608e5dea1086bc2c73\") predict(texts=texts, artifacts=artifacts) ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input parameter `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input texts to predict tags for. artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Predicted tags for each of the input texts. \"\"\" # Retrieve artifacts params = artifacts [ \"params\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] model = artifacts [ \"model\" ] # Prepare data preprocessed_texts = [ data . preprocess ( text , lower = bool ( strtobool ( str ( params . lower ))), # params.lower could be str/bool stem = bool ( strtobool ( str ( params . stem ))), ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts ), dtype = \"object\" ) y_filler = np . zeros (( len ( X ), len ( label_encoder ))) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"predict()"},{"location":"tagifai/train/","text":"Classes Trainer Object used to facilitate training. Methods eval_step ( self , dataloader ) Evaluation (val / test) step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def eval_step ( self , dataloader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs ) predict_step ( self , dataloader ) Prediction (inference) step. Note Loss is not calculated for this loop. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs ) train ( self , num_epochs , patience , train_dataloader , val_dataloader ) Training loop. Parameters: Name Type Description Default num_epochs int Maximum number of epochs to train for (can stop earlier based on performance). required patience int Number of acceptable epochs for continuous degrading performance. required train_dataloader DataLoader Dataloader object with training data split. required val_dataloader DataLoader Dataloader object with validation data split. required Exceptions: Type Description optuna.TrialPruned Early stopping of the optimization trial if poor performance. Returns: Type Description Tuple The best validation loss and the trained model from that point. Source code in tagifai/train.py def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model train_step ( self , dataloader ) Train step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def train_step ( self , dataloader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss Functions find_best_threshold ( y_true , y_prob ) Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true ndarray True labels. required y_prob ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in tagifai/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) params.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel ()) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )] train ( params , train_dataloader , val_dataloader , model , device , class_weights , trial = None ) Train a model. Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required train_dataloader DataLoader train data loader. required val_dataloader DataLoader val data loader. required model Module Initialize model to train. required device device Device to run model on. required class_weights Dict Dictionary of class weights. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Tuple The best trained model, loss and performance metrics. Source code in tagifai/train.py def train ( params : Namespace , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , model : nn . Module , device : torch . device , class_weights : Dict , trial : optuna . trial . _trial . Trial = None , ) -> Tuple : \"\"\"Train a model. Args: params (Namespace): Parameters for data processing and training. train_dataloader (torch.utils.data.DataLoader): train data loader. val_dataloader (torch.utils.data.DataLoader): val data loader. model (nn.Module): Initialize model to train. device (torch.device): Device to run model on. class_weights (Dict): Dictionary of class weights. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: The best trained model, loss and performance metrics. \"\"\" # Define loss class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) # Define optimizer & scheduler optimizer = torch . optim . Adam ( model . parameters (), lr = params . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( params . num_epochs , params . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) return params , best_model , best_val_loss","title":"Training"},{"location":"tagifai/train/#tagifai.train","text":"","title":"tagifai.train"},{"location":"tagifai/train/#classes","text":"","title":"Classes"},{"location":"tagifai/train/#tagifai.train.Trainer","text":"Object used to facilitate training.","title":"Trainer"},{"location":"tagifai/train/#methods","text":"","title":"Methods"},{"location":"tagifai/train/#functions","text":"","title":"Functions"},{"location":"tagifai/train/#tagifai.train.find_best_threshold","text":"Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true ndarray True labels. required y_prob ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in tagifai/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) params.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel ()) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )]","title":"find_best_threshold()"},{"location":"tagifai/train/#tagifai.train.train","text":"Train a model. Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required train_dataloader DataLoader train data loader. required val_dataloader DataLoader val data loader. required model Module Initialize model to train. required device device Device to run model on. required class_weights Dict Dictionary of class weights. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Tuple The best trained model, loss and performance metrics. Source code in tagifai/train.py def train ( params : Namespace , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , model : nn . Module , device : torch . device , class_weights : Dict , trial : optuna . trial . _trial . Trial = None , ) -> Tuple : \"\"\"Train a model. Args: params (Namespace): Parameters for data processing and training. train_dataloader (torch.utils.data.DataLoader): train data loader. val_dataloader (torch.utils.data.DataLoader): val data loader. model (nn.Module): Initialize model to train. device (torch.device): Device to run model on. class_weights (Dict): Dictionary of class weights. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: The best trained model, loss and performance metrics. \"\"\" # Define loss class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) # Define optimizer & scheduler optimizer = torch . optim . Adam ( model . parameters (), lr = params . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( params . num_epochs , params . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) return params , best_model , best_val_loss","title":"train()"},{"location":"tagifai/utils/","text":"Functions delete_experiment ( experiment_name ) Delete an experiment with name experiment_name . Parameters: Name Type Description Default experiment_name str Name of the experiment. required Source code in tagifai/utils.py def delete_experiment ( experiment_name : str ): \"\"\"Delete an experiment with name `experiment_name`. Args: experiment_name (str): Name of the experiment. \"\"\" client = mlflow . tracking . MlflowClient () experiment_id = client . get_experiment_by_name ( experiment_name ) . experiment_id client . delete_experiment ( experiment_id = experiment_id ) dict_diff ( d_a , d_b , d_a_name = 'a' , d_b_name = 'b' ) Differences between two dictionaries with numerical values. Parameters: Name Type Description Default d_a Dict Dictionary with data. required d_b Dict Dictionary to compare to. required d_a_name str Name of dict a. 'a' d_b_name str Name of dict b. 'b' Returns: Type Description Dict Source code in tagifai/utils.py def dict_diff ( d_a : Dict , d_b : Dict , d_a_name = \"a\" , d_b_name = \"b\" ) -> Dict : \"\"\"Differences between two dictionaries with numerical values. Args: d_a (Dict): Dictionary with data. d_b (Dict): Dictionary to compare to. d_a_name (str): Name of dict a. d_b_name (str): Name of dict b. Returns: Dict: Differences between keys with numerical values. \"\"\" # Recursively flatten d_a = pd . json_normalize ( d_a , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] d_b = pd . json_normalize ( d_b , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] if d_a . keys () != d_b . keys (): raise Exception ( \"Cannot compare these dictionaries because they have different keys.\" ) # Compare diff = {} for key in d_a : if isinstance ( d_a [ key ], numbers . Number ) and isinstance ( d_b [ key ], numbers . Number ): diff [ key ] = { d_a_name : d_a [ key ], d_b_name : d_b [ key ], \"diff\" : d_a [ key ] - d_b [ key ]} return diff list_to_dict ( list_of_dicts , key ) Convert a list of dict_a to a dict_b where the key in dict_b is an item in each dict_a . Parameters: Name Type Description Default list_of_dicts List list of items to convert to dict. required key str Name of the item in dict_a to use as primary key for dict_b . required Returns: Type Description Dict A dictionary with items from the list organized by key. Source code in tagifai/utils.py def list_to_dict ( list_of_dicts : List , key : str ) -> Dict : \"\"\"Convert a list of `dict_a` to a `dict_b` where the `key` in `dict_b` is an item in each `dict_a`. Args: list_of_dicts (List): list of items to convert to dict. key (str): Name of the item in `dict_a` to use as primary key for `dict_b`. Returns: A dictionary with items from the list organized by key. \"\"\" d_b = {} for d_a in list_of_dicts : d_b_key = d_a . pop ( key ) d_b [ d_b_key ] = d_a return d_b load_dict ( filepath ) Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in tagifai/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath ) as fp : d = json . load ( fp ) return d load_json_from_url ( url ) Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in tagifai/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data save_dict ( d , filepath , cls = None , sortkeys = False ) Save a dictionary to a specific location. Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required cls optional encoder to use on dict data. Defaults to None. None sortkeys bool sort keys in dict alphabetically. Defaults to False. False Source code in tagifai/utils.py def save_dict ( d : Dict , filepath : str , cls = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): sort keys in dict alphabetically. Defaults to False. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) set_device ( cuda ) Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in tagifai/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : # pragma: no cover, simple tensor type setting torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device set_seed ( seed = 1234 ) Set seed for reproducibility. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in tagifai/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducibility. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"Utilities"},{"location":"tagifai/utils/#tagifai.utils","text":"","title":"tagifai.utils"},{"location":"tagifai/utils/#functions","text":"","title":"Functions"},{"location":"tagifai/utils/#tagifai.utils.delete_experiment","text":"Delete an experiment with name experiment_name . Parameters: Name Type Description Default experiment_name str Name of the experiment. required Source code in tagifai/utils.py def delete_experiment ( experiment_name : str ): \"\"\"Delete an experiment with name `experiment_name`. Args: experiment_name (str): Name of the experiment. \"\"\" client = mlflow . tracking . MlflowClient () experiment_id = client . get_experiment_by_name ( experiment_name ) . experiment_id client . delete_experiment ( experiment_id = experiment_id )","title":"delete_experiment()"},{"location":"tagifai/utils/#tagifai.utils.dict_diff","text":"Differences between two dictionaries with numerical values. Parameters: Name Type Description Default d_a Dict Dictionary with data. required d_b Dict Dictionary to compare to. required d_a_name str Name of dict a. 'a' d_b_name str Name of dict b. 'b' Returns: Type Description Dict Source code in tagifai/utils.py def dict_diff ( d_a : Dict , d_b : Dict , d_a_name = \"a\" , d_b_name = \"b\" ) -> Dict : \"\"\"Differences between two dictionaries with numerical values. Args: d_a (Dict): Dictionary with data. d_b (Dict): Dictionary to compare to. d_a_name (str): Name of dict a. d_b_name (str): Name of dict b. Returns: Dict: Differences between keys with numerical values. \"\"\" # Recursively flatten d_a = pd . json_normalize ( d_a , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] d_b = pd . json_normalize ( d_b , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] if d_a . keys () != d_b . keys (): raise Exception ( \"Cannot compare these dictionaries because they have different keys.\" ) # Compare diff = {} for key in d_a : if isinstance ( d_a [ key ], numbers . Number ) and isinstance ( d_b [ key ], numbers . Number ): diff [ key ] = { d_a_name : d_a [ key ], d_b_name : d_b [ key ], \"diff\" : d_a [ key ] - d_b [ key ]} return diff","title":"dict_diff()"},{"location":"tagifai/utils/#tagifai.utils.list_to_dict","text":"Convert a list of dict_a to a dict_b where the key in dict_b is an item in each dict_a . Parameters: Name Type Description Default list_of_dicts List list of items to convert to dict. required key str Name of the item in dict_a to use as primary key for dict_b . required Returns: Type Description Dict A dictionary with items from the list organized by key. Source code in tagifai/utils.py def list_to_dict ( list_of_dicts : List , key : str ) -> Dict : \"\"\"Convert a list of `dict_a` to a `dict_b` where the `key` in `dict_b` is an item in each `dict_a`. Args: list_of_dicts (List): list of items to convert to dict. key (str): Name of the item in `dict_a` to use as primary key for `dict_b`. Returns: A dictionary with items from the list organized by key. \"\"\" d_b = {} for d_a in list_of_dicts : d_b_key = d_a . pop ( key ) d_b [ d_b_key ] = d_a return d_b","title":"list_to_dict()"},{"location":"tagifai/utils/#tagifai.utils.load_dict","text":"Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in tagifai/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath ) as fp : d = json . load ( fp ) return d","title":"load_dict()"},{"location":"tagifai/utils/#tagifai.utils.load_json_from_url","text":"Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in tagifai/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data","title":"load_json_from_url()"},{"location":"tagifai/utils/#tagifai.utils.save_dict","text":"Save a dictionary to a specific location. Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required cls optional encoder to use on dict data. Defaults to None. None sortkeys bool sort keys in dict alphabetically. Defaults to False. False Source code in tagifai/utils.py def save_dict ( d : Dict , filepath : str , cls = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): sort keys in dict alphabetically. Defaults to False. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys )","title":"save_dict()"},{"location":"tagifai/utils/#tagifai.utils.set_device","text":"Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in tagifai/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : # pragma: no cover, simple tensor type setting torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device","title":"set_device()"},{"location":"tagifai/utils/#tagifai.utils.set_seed","text":"Set seed for reproducibility. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in tagifai/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducibility. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"set_seed()"}]}